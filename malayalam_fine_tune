import os
import pandas as pd


BASE_DIR = "/content/drive/MyDrive"
UNZIPPED_DIR = os.path.join(BASE_DIR, "malayalam_fine_tune_unzipped", "malayalam_fine_tune")
OUTPUT_CSV = os.path.join(BASE_DIR, "manifest1000.csv")  

AUDIO_EXTENSIONS = [".wav", ".mp3", ".flac"]
samples = []


for root, _, files in os.walk(UNZIPPED_DIR):
    audio_files = [f for f in files if any(f.lower().endswith(ext) for ext in AUDIO_EXTENSIONS)]
    text_files = [f for f in files if f.lower().endswith(".txt")]

    if len(audio_files) == 1 and len(text_files) == 1:
        audio_path = os.path.join(root, audio_files[0])
        transcript_path = os.path.join(root, text_files[0])
        try:
            with open(transcript_path, "r", encoding="utf-8") as f:
                transcript = f.read().strip()
                if not transcript:
                    continue
                rel_path = os.path.relpath(audio_path, BASE_DIR).replace("\\", "/")
                samples.append({"audio_path": rel_path, "transcript": transcript})
        except Exception as e:
            print(f" Error reading in {root}: {e}")
    elif len(audio_files) > 1 or len(text_files) > 1:
        print(f" Skipping {root} (multiple audio or text files)")
    else:
        print(f" Skipping {root} (missing audio or transcript)")

print(f" Total valid pairs found: {len(samples)}")


df = pd.DataFrame(samples)
df.to_csv(OUTPUT_CSV, index=False, encoding="utf-8")

print(f" Saved {len(samples)} samples to: {OUTPUT_CSV}")


------------------------------------------------------------------

!pip install -q git+https://github.com/openai/whisper.git transformers datasets torchaudio

from transformers import WhisperProcessor, WhisperForConditionalGeneration


model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-small")
processor = WhisperProcessor.from_pretrained("openai/whisper-small")
---------------------------------------------------------


from transformers import Seq2SeqTrainingArguments

training_args = Seq2SeqTrainingArguments(
    output_dir=os.path.join(BASE_DIR, "finetuned_model_small"),
    per_device_train_batch_size=4, 
    gradient_accumulation_steps=2,
    learning_rate=1e-5,
    num_train_epochs=10,
    logging_steps=10,

    save_total_limit=1,
    report_to="none",
    fp16=True  
)
---------------------------------------------------------------
import pandas as pd
from datasets import Dataset

CSV_PATH = "/content/drive/MyDrive/manifest1000.csv"


df = pd.read_csv(CSV_PATH)

# Step 2: Convert to Hugging Face Dataset
dataset = Dataset.from_pandas(df)

# (Optional) shuffle for training
dataset = dataset.shuffle(seed=42)

print(" Loaded dataset with", len(dataset), "samples")
dataset[0]

----------------------------------------------------------------
import os
import torchaudio
from transformers import WhisperProcessor

BASE_DIR = "/content/drive/MyDrive"
processor = WhisperProcessor.from_pretrained("openai/whisper-small")

def prepare_sample(example):
    audio_path = os.path.join(BASE_DIR, example["audio_path"])
    waveform, sr = torchaudio.load(audio_path)

    
    input_features = processor.feature_extractor(
        waveform.squeeze(), sampling_rate=sr, return_tensors="pt"
    ).input_features[0]

    labels = processor.tokenizer(
        example["transcript"],
        return_tensors="pt",
        padding="longest",
        truncation=True,              
        max_length=448               
    ).input_ids[0]

    return {"input_features": input_features, "labels": labels}

dataset = dataset.map(prepare_sample)
------------------------------------------------------------------
from dataclasses import dataclass
from typing import Any, Dict, List
import torch

@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: Any

    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
        input_features = [{"input_features": f["input_features"]} for f in features]
        label_features = [{"input_ids": f["labels"]} for f in features]

        batch = self.processor.feature_extractor.pad(input_features, return_tensors="pt")
        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")

        batch["labels"] = labels_batch["input_ids"]
        return batch

data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)
------------------------------------------------------------------------------------
from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    data_collator=data_collator,
    tokenizer=processor.tokenizer  # no deprecation warning now
)

trainer.train()


----------------------------------------------------------------------------------
save_path = os.path.join(BASE_DIR, "finetuned_model_small")
model.save_pretrained(save_path)
processor.save_pretrained(save_path)
print("âœ… Fine-tuned model saved to:", save_path)
--------------------------------------------------------------------------------
!pip install flask flask-cors flask-ngrok
!pip install pyngrok

from pyngrok import ngrok, conf

conf.get_default().auth_token = "  "
-----------------------------------------------------------------------
from flask import Flask, request, jsonify
from flask_cors import CORS
import tempfile, os
import torchaudio
from transformers import WhisperProcessor, WhisperForConditionalGeneration, GenerationConfig
from pyngrok import ngrok


MODEL_PATH = "/content/drive/MyDrive/finetuned_model_small"
print("Loading fine-tuned Whisper-small model...")

model = WhisperForConditionalGeneration.from_pretrained(MODEL_PATH)
processor = WhisperProcessor.from_pretrained(MODEL_PATH)


model.config.forced_decoder_ids = None
model.generation_config = GenerationConfig()


app = Flask(__name__)
CORS(app)

@app.route("/transcribe", methods=["POST"])
def transcribe():
    print(" Received transcription request")

    if 'audio' not in request.files:
        return jsonify({"error": "No audio file provided"}), 400

    audio = request.files["audio"]
    with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as temp:
        audio.save(temp.name)
        temp_path = temp.name

    try:
        waveform, sr = torchaudio.load(temp_path)
        max_samples = sr * 10 
        waveform = waveform[:, :max_samples]

        if sr != 16000:
            waveform = torchaudio.transforms.Resample(sr, 16000)(waveform)
            sr = 16000

        inputs = processor.feature_extractor(
            waveform.squeeze(), sampling_rate=sr, return_tensors="pt"
        ).input_features

       
        forced_ids = processor.get_decoder_prompt_ids(language="ml", task="transcribe")


        generation_config = GenerationConfig(
            forced_decoder_ids=forced_ids,
            decoder_start_token_id=50258,
            max_length=448,
            suppress_tokens=[],
            begin_suppress_tokens=[],
        )


        ids = model.generate(inputs, generation_config=generation_config)
        transcription = processor.tokenizer.batch_decode(ids, skip_special_tokens=True)[0]

        print(" Transcription:", transcription)
        return jsonify({"transcription": transcription})

    except Exception as e:
        print(" Error:", str(e))
        return jsonify({"error": str(e)})
    finally:
        os.remove(temp_path)


public_url = ngrok.connect(5000)
print(f" Ngrok Tunnel URL: {public_url}")
app.run(port=5000)


